{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BMugo84/tensorflow-object-detection/blob/collab/tfod.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHilWQVcdG62"
      },
      "source": [
        "# Object Detection for Airtime Scratchcard Digits\n",
        "\n",
        "## Introduction\n",
        "Hello everyone! In this project, I will focus on the task of converting Safaricom and Telkom scratchcard airtime images into digits that can be used to load airtime. In the past, there was an app called \"RechargeKing\" that allowed users to load airtime by capturing images of scratchcards. However, recent changes in the fonts of Safaricom and Telkom scratch cards have rendered the app obsolete as the developers did not update it.\n",
        "\n",
        "## Problem Definition\n",
        "The problem I aim to solve is the digit recognition from scratchcard images. I want to create a solution that can automatically extract the airtime digits from these images, allowing users to easily load airtime without manual input.\n",
        "\n",
        "## Solution\n",
        "To tackle this problem, I will leverage Nicholas Renotte's TensorFlow Object Detection course, which provides a comprehensive guide on using TensorFlow for object detection. This course will serve as my foundation to build an object detection model capable of recognizing and extracting the digits from scratchcard images. I will use Python for coding and follow the principles outlined in the course.\n",
        "\n",
        "Let's get started!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZaaqtuhdG66"
      },
      "source": [
        "# Steps to Achieve Object Detection\n",
        "\n",
        "1. **Install and Setup**: We'll start by setting up our development environment and installing the necessary libraries, including TensorFlow.\n",
        "\n",
        "2. **Collect Images and Label**: Next, we'll gather a dataset of images that contains the objects we want to detect. We'll also label these objects to train our model effectively.\n",
        "\n",
        "3. **Training Models**: The heart of object detection is training models. I'll explain how to use pre-trained models or train custom ones to identify objects in images.\n",
        "\n",
        "4. **Detecting Objects**: Once our models are trained, we'll apply them to real-world images to detect and locate objects accurately.\n",
        "\n",
        "5. **Freezing and Conversion**: We'll discuss how to freeze and convert our trained model into a format suitable for deployment.\n",
        "\n",
        "6. **Performance Tuning**: Object detection isn't just about accuracy; it's also about performance. We'll optimize our models for speed and efficiency.\n",
        "\n",
        "7. **Packaging**: Finally, I'll guide you on packaging your object detection solution for practical use.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7fyTBqgdG66"
      },
      "source": [
        "I plan on doing most of the work here then the heavy lifting will be done on collab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMqbLpJwdG66"
      },
      "source": [
        "# 1. **Install and Setup**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToeUDsYTdG67"
      },
      "outputs": [],
      "source": [
        "# # create venv environment\n",
        "# !python -m venv tfod\n",
        "\n",
        "# # activate tfod env\n",
        "# !source tfod/bin/activate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JcJ5BS4dG68"
      },
      "outputs": [],
      "source": [
        "# # upgrade pip\n",
        "# !python -m pip install --upgrade pip\n",
        "\n",
        "# # install ipykernel\n",
        "# !pip install ipykernel\n",
        "\n",
        "# # assosiate ipykernel to tfod env where name is the name of your virtual environment\n",
        "# !python -m ipykernel install --user --name=tfod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_191BsZdG68"
      },
      "source": [
        "\n",
        "# 2. **Collect Images and Label**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCD_Pf_CdG68"
      },
      "source": [
        "We will be using a number of scratchcard pictures(already used) and label them. right now i have about 50 so they should suffice\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ir2jLtmZdG69",
        "outputId": "1c530bde-34b1-42b0-a43d-d276ab9e5ec4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in c:\\users\\bm\\tensorflow-object-detection\\tfod\\lib\\site-packages (4.8.1.78)\n",
            "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\bm\\tensorflow-object-detection\\tfod\\lib\\site-packages (from opencv-python) (1.26.0)\n"
          ]
        }
      ],
      "source": [
        "# install opencv\n",
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "b1cSCKVEdG6-"
      },
      "outputs": [],
      "source": [
        "# import opencv\n",
        "import cv2\n",
        "\n",
        "# import uuid\n",
        "import uuid\n",
        "\n",
        "# import os\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAxdX-WPdG6-"
      },
      "source": [
        "the IMAGES_PATH will host our images and corresponding labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uqh4mfamdG6-"
      },
      "outputs": [],
      "source": [
        "# make a folder for images\n",
        "IMAGES_PATH =  os.path.join('tensorflow','workspace','images', 'collectedimages')\n",
        "\n",
        "if not os.path.exists(IMAGES_PATH):\n",
        "    if os.name == 'posix':\n",
        "        !mkdir -p {IMAGES_PATH}\n",
        "    if os.name == 'nt':\n",
        "        !mkdir {IMAGES_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qaUFw6IdG6-"
      },
      "source": [
        "I used 40 images of scratch cards for labeling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUxG6DaGdG6-"
      },
      "source": [
        "## 2.1 Label images using [labelImg](https://github.com/HumanSignal/labelImg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rju6trCHdG6_"
      },
      "source": [
        "install pyqt5 for labelimg gui rendering and lxml for handling xml templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VI7WebIdG6_"
      },
      "outputs": [],
      "source": [
        "# install pyqt5 lxml\n",
        "!pip install --upgrade pyqt5 lxml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iXwAB1Q7dG6_"
      },
      "outputs": [],
      "source": [
        "# create a folder where the labelimg repo will be clones into\n",
        "LABELIMG_PATH = os.path.join('tensorflow', 'labelimg')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYJVbJ8FdG6_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# clone the git repo into the new folder about 200MB\n",
        "if not os.path.exists(LABELIMG_PATH):\n",
        "    !mkdir {LABELIMG_PATH}\n",
        "    !git clone https://github.com/HumanSignal/labelImg.git {LABELIMG_PATH}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nfqp1kfdG6_"
      },
      "outputs": [],
      "source": [
        "# get into the folder and run the command shown\n",
        "if os.name == 'posix':\n",
        "    !cd {LABELIMG_PATH} && make qt5py3\n",
        "if os.name == 'nt':\n",
        "    !cd {LABELIMG_PATH} && pyrcc5 -o libs/resources.py resources.qrc\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apyvVnVLdG7A",
        "outputId": "4391ff67-bedb-4fe7-a4f0-bf7cda12b77f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image:C:\\Users\\BM\\tensorflow-object-detection\\tensorflow\\workspace\\images\\collectedimages\\IMG_20231009_203326.jpg -> Annotation:C:/Users/BM/tensorflow-object-detection/tensorflow/workspace/images/collectedimages\\IMG_20231009_203326.xml\n",
            "Image:C:\\Users\\BM\\tensorflow-object-detection\\tensorflow\\workspace\\images\\collectedimages\\IMG_20231009_203333.jpg -> Annotation:C:/Users/BM/tensorflow-object-detection/tensorflow/workspace/images/collectedimages\\IMG_20231009_203333.xml\n",
            "Image:C:\\Users\\BM\\tensorflow-object-detection\\tensorflow\\workspace\\images\\collectedimages\\IMG_20231009_203340.jpg -> Annotation:C:/Users/BM/tensorflow-object-detection/tensorflow/workspace/images/collectedimages\\IMG_20231009_203340.xml\n",
            "Image:C:\\Users\\BM\\tensorflow-object-detection\\tensorflow\\workspace\\images\\collectedimages\\IMG_20231009_203347.jpg -> Annotation:C:/Users/BM/tensorflow-object-detection/tensorflow/workspace/images/collectedimages\\IMG_20231009_203347.xml\n",
            "Image:C:\\Users\\BM\\tensorflow-object-detection\\tensorflow\\workspace\\images\\collectedimages\\IMG_20231009_203359.jpg -> Annotation:C:/Users/BM/tensorflow-object-detection/tensorflow/workspace/images/collectedimages\\IMG_20231009_203359.xml\n",
            "Image:C:\\Users\\BM\\tensorflow-object-detection\\tensorflow\\workspace\\images\\collectedimages\\IMG_20231009_203406.jpg -> Annotation:C:/Users/BM/tensorflow-object-detection/tensorflow/workspace/images/collectedimages\\IMG_20231009_203406.xml\n",
            "Cancel creation.\n",
            "Image:C:\\Users\\BM\\tensorflow-object-detection\\tensorflow\\workspace\\images\\collectedimages\\IMG_20231009_203420.jpg -> Annotation:C:/Users/BM/tensorflow-object-detection/tensorflow/workspace/images/collectedimages\\IMG_20231009_203420.xml\n",
            "Image:C:\\Users\\BM\\tensorflow-object-detection\\tensorflow\\workspace\\images\\collectedimages\\IMG_20231009_203430.jpg -> Annotation:C:/Users/BM/tensorflow-object-detection/tensorflow/workspace/images/collectedimages\\IMG_20231009_203430.xml\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-10-10T19:47:55.777ZE [7376:ShellIpcClient] shell_ipc_client.cc:138:Connect Can't connect to socket at: \\\\.\\Pipe\\GoogleDriveFSPipe_BM_shell\n",
            "2023-10-10T19:47:55.812ZE [7376:ShellIpcClient] shell_ipc_client.cc:621:operator() Failed to connect to the server: NOT_FOUND: Can't connect to socket at: \\\\.\\Pipe\\GoogleDriveFSPipe_BM_shell [type.googleapis.com/drive.ds.Status='UNAVAILABLE_RESOURCE']\n",
            "=== Source Location Trace: ===\n",
            "apps/drive/fs/ipc/shell_ipc_client.cc:139\n",
            "\n",
            "2023-10-10T19:47:55.812ZE [16936:ShellIpcClient] shell_ipc_client.cc:138:Connect Can't connect to socket at: \\\\.\\Pipe\\GoogleDriveFSPipe_BM_shell\n",
            "2023-10-10T19:47:55.812ZE [16936:ShellIpcClient] shell_ipc_client.cc:621:operator() Failed to connect to the server: NOT_FOUND: Can't connect to socket at: \\\\.\\Pipe\\GoogleDriveFSPipe_BM_shell [type.googleapis.com/drive.ds.Status='UNAVAILABLE_RESOURCE']\n",
            "=== Source Location Trace: ===\n",
            "apps/drive/fs/ipc/shell_ipc_client.cc:139\n",
            "\n",
            "2023-10-10T19:47:55.812ZE [7672:ShellIpcClient] shell_ipc_client.cc:138:Connect Can't connect to socket at: \\\\.\\Pipe\\GoogleDriveFSPipe_BM_shell\n",
            "2023-10-10T19:47:55.812ZE [7672:ShellIpcClient] shell_ipc_client.cc:621:operator() Failed to connect to the server: NOT_FOUND: Can't connect to socket at: \\\\.\\Pipe\\GoogleDriveFSPipe_BM_shell [type.googleapis.com/drive.ds.Status='UNAVAILABLE_RESOURCE']\n",
            "=== Source Location Trace: ===\n",
            "apps/drive/fs/ipc/shell_ipc_client.cc:139\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# open the app\n",
        "!cd {LABELIMG_PATH} && python labelImg.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ4OO3uEdG7A"
      },
      "source": [
        "typical labeling\n",
        "\n",
        "![Screenshot 2023-10-10 230307.png](<attachment:Screenshot 2023-10-10 230307.png>)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy9YYLzSdG7A"
      },
      "source": [
        "## **2.2 move them into a training and testing partition**\n",
        "\n",
        "cretate two folders under images. one for training and one for testing.\n",
        "we will split them 80/20 where we have 32 images for training and 8 images for testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MCCanuIjdG7A"
      },
      "outputs": [],
      "source": [
        "TRAINING_PATH = os.path.join('tensorflow', 'workspace','images', 'train')\n",
        "TESTING_PATH = os.path.join('tensorflow', 'workspace','images', 'test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TVfdwHy7dG7A"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(TRAINING_PATH) or not os.path.exists(TESTING_PATH):\n",
        "    if os.name == 'posix':\n",
        "        !mkdir -p {TRAINING_PATH}\n",
        "        !mkdir -p {TESTING_PATH}\n",
        "    if os.name == 'nt':\n",
        "        !mkdir {TRAINING_PATH}\n",
        "        !mkdir {TESTING_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVO9oPJZdG7B"
      },
      "source": [
        "copy-paste images plus respective annotations to train and test folders"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# since they are all in local machine, we will upload them from google drive\n",
        "#/content/tensorflow/workspace/images   /content/drive/MyDrive/tensorflow-object-detection-main/tensorflow/workspace/images\n",
        "!cp -r /content/drive/MyDrive/tensorflow-object-detection-main/tensorflow/workspace/images /content/tensorflow/workspace/\n"
      ],
      "metadata": {
        "id": "Wm4bezTIgPZg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/tensorflow/workspace/images/test/\n"
      ],
      "metadata": {
        "id": "jEbNCJWmiTtM",
        "outputId": "07f367ef-6f2c-42f0-8975-164daea4adb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMG_20231009_203326.jpg  IMG_20231009_203340.jpg  IMG_20231009_203359.jpg  IMG_20231009_203420.jpg\n",
            "IMG_20231009_203326.xml  IMG_20231009_203340.xml  IMG_20231009_203359.xml  IMG_20231009_203420.xml\n",
            "IMG_20231009_203333.jpg  IMG_20231009_203347.jpg  IMG_20231009_203406.jpg  IMG_20231009_203430.jpg\n",
            "IMG_20231009_203333.xml  IMG_20231009_203347.xml  IMG_20231009_203406.xml  IMG_20231009_203430.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "OInWDX1qhN78",
        "outputId": "953e00a9-0af3-479c-cc0d-82edae9583ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgBV1rzSdG7B"
      },
      "source": [
        "\n",
        "# 3. **Training Models**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Hc4BitYdG7B"
      },
      "source": [
        "### **Training**\n",
        "In order to leverage a custom TFOD model, we need to finetune /train a new computer vision model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fE5sQ1NdG7B"
      },
      "source": [
        "### **Evaluation**\n",
        "Once you have trained a model, ideally you want to take a look at how well it's performing. we normally take a look at:\n",
        "\n",
        "Precision: TP/(TP+FP)\n",
        "*what propotion of my detections were correct?*\n",
        "\n",
        "Recall: TP/ (TP+FN)\n",
        "*what propotion of the actual objects did I capture?*\n",
        "\n",
        "Loss:\n",
        "*How well is the model performing against the data provided?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU3lVIM7dG7B"
      },
      "source": [
        "# TensorFlow Model Zoo: Making Model Choices\n",
        "\n",
        "Within the TensorFlow Model Zoo, you'll find a diverse array of models at your disposal, each designed for specific tasks. However, when making your selection, it's crucial to understand the trade-off that often comes into play.\n",
        "\n",
        "In the realm of object detection and beyond, the rule of thumb is simple:\n",
        "\n",
        "- Opting for faster detections often means sacrificing some degree of accuracy.\n",
        "- On the flip side, if you prioritize higher accuracy, be prepared for slightly slower detection times.\n",
        "\n",
        "Balancing these factors is key to choosing the right model for your particular use case. In the following sections, we'll delve deeper into the considerations that guide your model selection within the TensorFlow Model Zoo.\n",
        "\n",
        "\n",
        "Current choices for this project:\n",
        "\n",
        "1. **SSD MobileNet V2 FPNLite 640x640**:\n",
        "   - Speed: Relatively fast due to MobileNet architecture.\n",
        "   - Accuracy: 28.2 mAP (mean Average Precision), which is moderate.\n",
        "   - Resolution: 640x640.\n",
        "   - Suitable for scenarios where speed is more critical than very high accuracy.\n",
        "\n",
        "2. **SSD ResNet50 V1 FPN 640x640 (RetinaNet50)**:\n",
        "   - Speed: Moderate speed due to ResNet50 backbone.\n",
        "   - Accuracy: 34.3 mAP, which is relatively good.\n",
        "   - Resolution: 640x640.\n",
        "   - A balanced choice for accuracy and speed.\n",
        "\n",
        "3. **SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)**:\n",
        "   - Speed: Slower than the 640x640 version.\n",
        "   - Accuracy: 38.3 mAP, which is better.\n",
        "   - Higher resolution (1024x1024) can provide better accuracy but at the cost of speed.\n",
        "   - Choose this if you need higher accuracy and have the hardware to support it.\n",
        "\n",
        "4. **SSD ResNet101 V1 FPN 640x640 (RetinaNet101)**:\n",
        "   - Speed: Similar to ResNet50.\n",
        "   - Accuracy: 35.6 mAP, which is quite good.\n",
        "   - Resolution: 640x640.\n",
        "   - Offers a good balance of speed and accuracy, similar to the RetinaNet50 models.\n",
        "\n",
        "5. **SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)**:\n",
        "   - Speed: Slower than the 640x640 version.\n",
        "   - Accuracy: 39.5 mAP, which is the highest among the models listed.\n",
        "   - Resolution: 1024x1024.\n",
        "   - Opt for this if you require the highest accuracy and have the necessary hardware capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7xz9sIFdG7B"
      },
      "source": [
        "When choosing an object detection model, there are several critical factors to consider:\n",
        "\n",
        "1. **Deployment Platform**: Think about where you intend to deploy this model. Will it run on a computer, a smartphone, a Raspberry Pi, or a different device? The hardware resources available on the deployment platform will impact your choice.\n",
        "\n",
        "2. **Frame Processing Time**: Consider the time you're willing to allocate for processing each frame. Depending on your use case, you may require real-time or near-real-time processing. Faster models can be essential for applications where speed is of the essence.\n",
        "\n",
        "3. **Accuracy Requirements**: Evaluate how accurate your model needs to be. In some cases, high accuracy is crucial, while in others, a slightly lower accuracy may be acceptable if it allows for faster processing.\n",
        "\n",
        "Taking these factors into account, if you prioritize faster processing and your deployment platform has limited resources, `SSD MobileNet V2 FPNLite 640x640` could be an excellent choice. It strikes a balance between speed and accuracy, making it suitable for scenarios where real-time or near-real-time processing is desired.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJiacWcUdG7C"
      },
      "source": [
        "## 3.1 **Setup paths**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pZ6bs6OTdG7C"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVqgTPJAdG7C"
      },
      "source": [
        "we are going to download `SSD MobileNet V2 FPNLite 640x640` and place it in our folder. this arrangement will be useful in collab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "p-uHCuaIdG7C"
      },
      "outputs": [],
      "source": [
        "CUSTOM_MODEL_NAME = 'my_ssd_mobnet'\n",
        "PRETRAINED_MODEL_NAME = 'ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8'\n",
        "PRETRAINED_MODEL_URL = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz'\n",
        "TF_RECORD_SCRIPT_NAME = 'generate_tfrecord.py'\n",
        "LABEL_MAP_NAME = 'label_map.pbtxt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JGhCjvoudG7C"
      },
      "outputs": [],
      "source": [
        "paths = {\n",
        "    'WORKSPACE_PATH': os.path.join('tensorflow', 'workspace'),\n",
        "    'SCRIPTS_PATH': os.path.join('tensorflow', 'scripts'),\n",
        "    'APIMODEL_PATH': os.path.join('tensorflow', 'models'),\n",
        "    'ANNOTATION_PATH': os.path.join('tensorflow', 'workspace', 'annotations'),\n",
        "    'IMAGE_PATH': os.path.join('tensorflow', 'workspace', 'images'),\n",
        "    'MODEL_PATH': os.path.join('tensorflow', 'workspace', 'models'),\n",
        "    'PRETRAINED_MODEL_PATH': os.path.join('tensorflow', 'workspace', 'pretrained_models'),\n",
        "    'CHECKPOINT_PATH': os.path.join('tensorflow', 'workspace','models', CUSTOM_MODEL_NAME),\n",
        "    'OUTPUT_PATH': os.path.join('tensorflow', 'workspace','models', CUSTOM_MODEL_NAME, 'export'),\n",
        "    'TFJS_PATH': os.path.join('tensorflow', 'workspace','models', CUSTOM_MODEL_NAME, 'tfjsexport'),\n",
        "    'TFLITE_PATH': os.path.join('tensorflow', 'workspace','models', CUSTOM_MODEL_NAME, 'tfliteexport'),\n",
        "    'PROTOC_PATH': os.path.join('tensorflow', 'workspace')\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wKM74UabdG7D"
      },
      "outputs": [],
      "source": [
        "files = {\n",
        "    'PIPELINE_CONFIG': os.path.join('tensorflow', 'workspace', 'models', CUSTOM_MODEL_NAME, 'pipeline.config'),\n",
        "    'TF_RECORD_SCRIPT': os.path.join(paths['SCRIPTS_PATH'], TF_RECORD_SCRIPT_NAME),\n",
        "    'LABELMAP': os.path.join(paths['ANNOTATION_PATH'], LABEL_MAP_NAME)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tDnJUA2HdG7D"
      },
      "outputs": [],
      "source": [
        "for path in paths.values():\n",
        "    if not os.path.exists(path):\n",
        "        if os.name == 'posix':\n",
        "            !mkdir -p {path}\n",
        "        if os.name == 'nt':\n",
        "            !mkdir {path}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYBL57wwdG7D"
      },
      "source": [
        "## 3.2 **Download TF Models Pretrained Models from TensorFlow Model Zoo and Install TFOD.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMOnveCcdG7D"
      },
      "outputs": [],
      "source": [
        "# https://github.com/tensorflow/models/tree/master"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ug67ynVrdG7I",
        "outputId": "8931de0d-009a-4a94-b0d2-f6f238a9e9e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.13.0 in /usr/local/lib/python3.10/dist-packages (2.13.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.59.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.9.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (16.0.6)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.34.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.13.0) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.0.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "# due to unexpected and irreversible errors, downgrade to tensorflow 2.13.0\n",
        "!pip install tensorflow==2.13.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jeH6dG9ZdG7I"
      },
      "outputs": [],
      "source": [
        "# install wget\n",
        "if os.name=='nt':\n",
        "    !pip install wget\n",
        "    import wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jaY-28aSdG7J",
        "outputId": "b5dc13d9-5641-4d76-f499-28bc6dab2a8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'tensorflow/models'...\n",
            "remote: Enumerating objects: 87954, done.\u001b[K\n",
            "remote: Counting objects: 100% (1666/1666), done.\u001b[K\n",
            "remote: Compressing objects: 100% (690/690), done.\u001b[K\n",
            "remote: Total 87954 (delta 1066), reused 1518 (delta 960), pack-reused 86288\u001b[K\n",
            "Receiving objects: 100% (87954/87954), 601.15 MiB | 27.88 MiB/s, done.\n",
            "Resolving deltas: 100% (62964/62964), done.\n"
          ]
        }
      ],
      "source": [
        "# clone the tfod garden into APIMODEL_PATH\n",
        "if not os.path.exists(os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection')):\n",
        "    !git clone https://github.com/tensorflow/models {paths['APIMODEL_PATH']}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/setup.py /content/tensorflow/models/research/object_detection/packages/tf2"
      ],
      "metadata": {
        "id": "YIlPa45fwvZC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "jT4dZwRjdG7J",
        "outputId": "cb1eebfe-3407-4fbe-b0ff-ea11e2f8bdfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "protobuf-compiler is already the newest version (3.12.4-1ubuntu7.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n",
            "Processing /content/tensorflow/models/research\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting avro-python3 (from object-detection==0.1)\n",
            "  Using cached avro-python3-1.10.2.tar.gz (38 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting apache-beam (from object-detection==0.1)\n",
            "  Using cached apache_beam-2.51.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.7 MB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (9.4.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (4.9.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (3.7.1)\n",
            "Collecting Cython==3.0.0 (from object-detection==0.1)\n",
            "  Using cached Cython-3.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (21.6.0)\n",
            "Requirement already satisfied: tf-slim in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (1.16.0)\n",
            "Collecting pycocotools==2.0.6 (from object-detection==0.1)\n",
            "  Using cached pycocotools-2.0.6.tar.gz (24 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting lvis (from object-detection==0.1)\n",
            "  Using cached lvis-0.5.3-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (1.11.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (1.5.3)\n",
            "Requirement already satisfied: tensorflow==2.13.0 in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (2.13.0)\n",
            "Requirement already satisfied: tf-models-official==2.13.2 in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (2.13.2)\n",
            "Collecting tensorflow-io==0.32.0 (from object-detection==0.1)\n",
            "  Using cached tensorflow_io-0.32.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (28.0 MB)\n",
            "Requirement already satisfied: keras==2.13.1 in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (2.13.1)\n",
            "Collecting pyparsing==2.4.7 (from object-detection==0.1)\n",
            "  Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "Collecting sacrebleu<=2.2.0 (from object-detection==0.1)\n",
            "  Using cached sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pycocotools==2.0.6->object-detection==0.1) (1.23.5)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0->object-detection==0.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0->object-detection==0.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0->object-detection==0.1) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0->object-detection==0.1) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0->object-detection==0.1) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0->object-detection==0.1) (1.59.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0->object-detection==0.1) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0->object-detection==0.1) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0->object-detection==0.1) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0->object-detection==0.1) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0->object-detection==0.1) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0->object-detection==0.1) (67.7.2)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0->object-detection==0.1) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0->object-detection==0.1) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0->object-detection==0.1) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0->object-detection==0.1) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0->object-detection==0.1) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0->object-detection==0.1) (0.34.0)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow==2.13.0->object-detection==0.1)\n",
            "  Using cached tensorflow_io_gcs_filesystem-0.32.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.2->object-detection==0.1) (0.5.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.2->object-detection==0.1) (2.84.0)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.2->object-detection==0.1) (3.0.0)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.2->object-detection==0.1) (1.5.16)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.2->object-detection==0.1) (4.1.3)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.2->object-detection==0.1) (4.8.1.78)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.2->object-detection==0.1) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.2->object-detection==0.1) (9.0.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.2->object-detection==0.1) (6.0.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.2->object-detection==0.1) (0.1.99)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.2->object-detection==0.1) (1.2.2)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.2->object-detection==0.1) (4.9.3)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.2->object-detection==0.1) (0.15.0)\n",
            "Requirement already satisfied: tensorflow-model-optimization>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.2->object-detection==0.1) (0.7.5)\n",
            "Requirement already satisfied: tensorflow-text~=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.13.2->object-detection==0.1) (2.13.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->object-detection==0.1) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->object-detection==0.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->object-detection==0.1) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->object-detection==0.1) (1.4.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->object-detection==0.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->object-detection==0.1) (2023.3.post1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (2.8.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (2023.6.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (0.4.6)\n",
            "Collecting crcmod<2.0,>=1.7 (from apache-beam->object-detection==0.1)\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting orjson<4,>=3.9.7 (from apache-beam->object-detection==0.1)\n",
            "  Downloading orjson-3.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m446.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1 (from apache-beam->object-detection==0.1)\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cloudpickle~=2.2.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (2.2.1)\n",
            "Collecting fastavro<2,>=0.23.6 (from apache-beam->object-detection==0.1)\n",
            "  Downloading fastavro-1.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasteners<1.0,>=0.3 (from apache-beam->object-detection==0.1)\n",
            "  Downloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam->object-detection==0.1)\n",
            "  Downloading hdfs-2.7.3.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (0.22.0)\n",
            "Collecting js2py<1,>=0.74 (from apache-beam->object-detection==0.1)\n",
            "  Downloading Js2Py-0.74-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting objsize<0.7.0,>=0.6.1 (from apache-beam->object-detection==0.1)\n",
            "  Downloading objsize-0.6.1-py3-none-any.whl (9.3 kB)\n",
            "Collecting pymongo<5.0.0,>=3.8.0 (from apache-beam->object-detection==0.1)\n",
            "  Downloading pymongo-4.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (671 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m671.3/671.3 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (1.22.3)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (1.4.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (2.31.0)\n",
            "Collecting zstandard<1,>=0.18.0 (from apache-beam->object-detection==0.1)\n",
            "  Downloading zstandard-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow<12.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (9.0.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.10/dist-packages (from lvis->object-detection==0.1) (4.8.0.76)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.13.0->object-detection==0.1) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.13.2->object-detection==0.1) (2.17.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.13.2->object-detection==0.1) (0.1.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.13.2->object-detection==0.1) (2.11.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.13.2->object-detection==0.1) (4.1.1)\n",
            "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam->object-detection==0.1)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.10/dist-packages (from js2py<1,>=0.74->apache-beam->object-detection==0.1) (5.1)\n",
            "Collecting pyjsparser>=2.5.1 (from js2py<1,>=0.74->apache-beam->object-detection==0.1)\n",
            "  Downloading pyjsparser-2.7.1.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.13.2->object-detection==0.1) (2023.7.22)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.13.2->object-detection==0.1) (4.66.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.13.2->object-detection==0.1) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.13.2->object-detection==0.1) (2.0.6)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.13.2->object-detection==0.1) (6.1.0)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo<5.0.0,>=3.8.0->apache-beam->object-detection==0.1)\n",
            "  Downloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0->object-detection==0.1) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0->object-detection==0.1) (3.5)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0->object-detection==0.1) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0->object-detection==0.1) (3.0.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official==2.13.2->object-detection==0.1) (0.1.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official==2.13.2->object-detection==0.1) (0.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official==2.13.2->object-detection==0.1) (0.3.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official==2.13.2->object-detection==0.1) (4.9)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval->tf-models-official==2.13.2->object-detection==0.1) (1.2.2)\n",
            "Requirement already satisfied: array-record in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.13.2->object-detection==0.1) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.13.2->object-detection==0.1) (8.1.7)\n",
            "Requirement already satisfied: etils[enp,epath,etree]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.13.2->object-detection==0.1) (1.5.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.13.2->object-detection==0.1) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.13.2->object-detection==0.1) (1.14.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.13.2->object-detection==0.1) (0.10.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official==2.13.2->object-detection==0.1) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official==2.13.2->object-detection==0.1) (6.1.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official==2.13.2->object-detection==0.1) (3.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.13.2->object-detection==0.1) (1.60.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official==2.13.2->object-detection==0.1) (5.3.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0->object-detection==0.1) (1.3.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.13.2->object-detection==0.1) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.13.2->object-detection==0.1) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow==2.13.0->object-detection==0.1) (2.1.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle>=1.3.9->tf-models-official==2.13.2->object-detection==0.1) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official==2.13.2->object-detection==0.1) (1.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0->object-detection==0.1) (3.2.2)\n",
            "Building wheels for collected packages: object-detection, pycocotools, avro-python3, crcmod, dill, hdfs, pyjsparser, docopt\n",
            "  Building wheel for object-detection (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1697375 sha256=d0cfd3e59d138b5a0bab3220bbbb04222f6239827d8102184f8441a0fcb3be56\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-y8mlo4zc/wheels/98/68/ba/cb9e30e71245301cb10375c6d9cd6e721173a2637f378668be\n",
            "  Building wheel for pycocotools (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0.6-cp310-cp310-linux_x86_64.whl size=376900 sha256=34ad992148ae15580e452805c4189984d41313a3044e9626f62b9cceb1c5dc9f\n",
            "  Stored in directory: /root/.cache/pip/wheels/58/e6/f9/f87c8f8be098b51b616871315318329cae12cdb618f4caac93\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.10.2-py3-none-any.whl size=43991 sha256=4aa7ee9932a645c7b8b36231de34b518017bdada34d651f79fc4b2a40e2819fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/85/62/6cdd81c56f923946b401cecff38055b94c9b766927f7d8ca82\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp310-cp310-linux_x86_64.whl size=31402 sha256=8e62162f4498ed68fa121a7cfa5b81622869780cc343ba064b6616801a84ac4d\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/4c/07/72215c529bd59d67e3dac29711d7aba1b692f543c808ba9e86\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78542 sha256=df914a57dfc5d67f75c2c11e675b43884bbb8aec92e202e34f2796b7dc5961ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/e2/86/64980d90e297e7bf2ce588c2b96e818f5399c515c4bb8a7e4f\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34325 sha256=a6e2950aa007786c770ac542cbeb95278cb4ba2b497cd513fc6039b7139dd403\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/8d/b6/99c1c0a3ac5788c866b0ecd3f48b0134a5910e6ed26011800b\n",
            "  Building wheel for pyjsparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyjsparser: filename=pyjsparser-2.7.1-py3-none-any.whl size=25982 sha256=562b32b280229680517df272bd55d9426e39c0f4fad4def6020daa8c6628decd\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/81/26/5956478df303e2bf5a85a5df595bb307bd25948a4bab69f7c7\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=69a1b8ae9dc2cfa3dc2619c72b9a72edfed9f340b0e94c66ea6376283a857150\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built object-detection pycocotools avro-python3 crcmod dill hdfs pyjsparser docopt\n",
            "Installing collected packages: pyjsparser, docopt, crcmod, zstandard, tensorflow-io-gcs-filesystem, sacrebleu, pyparsing, orjson, objsize, js2py, fasteners, fastavro, dnspython, dill, Cython, avro-python3, tensorflow-io, pymongo, hdfs, pycocotools, lvis, apache-beam, object-detection\n",
            "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
            "    Found existing installation: tensorflow-io-gcs-filesystem 0.34.0\n",
            "    Uninstalling tensorflow-io-gcs-filesystem-0.34.0:\n",
            "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.34.0\n",
            "  Attempting uninstall: sacrebleu\n",
            "    Found existing installation: sacrebleu 2.3.1\n",
            "    Uninstalling sacrebleu-2.3.1:\n",
            "      Successfully uninstalled sacrebleu-2.3.1\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.1.1\n",
            "    Uninstalling pyparsing-3.1.1:\n",
            "      Successfully uninstalled pyparsing-3.1.1\n",
            "  Attempting uninstall: Cython\n",
            "    Found existing installation: Cython 3.0.3\n",
            "    Uninstalling Cython-3.0.3:\n",
            "      Successfully uninstalled Cython-3.0.3\n",
            "  Attempting uninstall: pycocotools\n",
            "    Found existing installation: pycocotools 2.0.7\n",
            "    Uninstalling pycocotools-2.0.7:\n",
            "      Successfully uninstalled pycocotools-2.0.7\n",
            "Successfully installed Cython-3.0.0 apache-beam-2.51.0 avro-python3-1.10.2 crcmod-1.7 dill-0.3.1.1 dnspython-2.4.2 docopt-0.6.2 fastavro-1.8.4 fasteners-0.19 hdfs-2.7.3 js2py-0.74 lvis-0.5.3 object-detection-0.1 objsize-0.6.1 orjson-3.9.9 pycocotools-2.0.6 pyjsparser-2.7.1 pymongo-4.5.0 pyparsing-2.4.7 sacrebleu-2.2.0 tensorflow-io-0.32.0 tensorflow-io-gcs-filesystem-0.32.0 zstandard-0.21.0\n"
          ]
        }
      ],
      "source": [
        "# install tensorflow object detection\n",
        "if os.name=='posix':\n",
        "    !apt-get install protobuf-compiler\n",
        "    !cd tensorflow/models/research && protoc object_detection/protos/*.proto --python_out=. && cp object_detection/packages/tf2/setup.py . && python -m pip install .\n",
        "\n",
        "if os.name=='nt':\n",
        "    url=\"https://github.com/protocolbuffers/protobuf/releases/download/v3.15.6/protoc-3.15.6-win64.zip\"\n",
        "    wget.download(url)\n",
        "    !move protoc-3.15.6-win64.zip {paths['PROTOC_PATH']}\n",
        "    !cd {paths['PROTOC_PATH']} && tar -xf protoc-3.15.6-win64.zip\n",
        "    os.environ['PATH'] += os.pathsep + os.path.abspath(os.path.join(paths['PROTOC_PATH'], 'bin'))\n",
        "    !cd tensorflow/models/research && protoc object_detection/protos/*.proto --python_out=. && copy object_detection\\\\packages\\\\tf2\\\\setup.py setup.py && python setup.py build && python setup.py install\n",
        "    !cd tensorflow/models/research/slim && pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "GHN22R6NdG7J",
        "outputId": "b148fd5b-a3aa-4105-f831-77471a6ab097",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-16 19:05:43.480041: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-16 19:05:44.840791: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/usr/local/lib/python3.10/dist-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
            "caused by: ['/usr/local/lib/python3.10/dist-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6Status12empty_stringB5cxx11Ev']\n",
            "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/usr/local/lib/python3.10/dist-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
            "caused by: ['/usr/local/lib/python3.10/dist-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZNK10tensorflow4data11DatasetBase8FinalizeEPNS_15OpKernelContextESt8functionIFN3tsl8StatusOrISt10unique_ptrIS1_NS5_4core15RefCountDeleterEEEEvEE']\n",
            "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n",
            "Running tests under Python 3.10.12: /usr/bin/python3\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
            "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n",
            "W1016 19:05:49.260990 132297464188928 batch_normalization.py:1531] `tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n",
            "W1016 19:05:49.835353 132297464188928 model_builder.py:1112] Building experimental DeepMAC meta-arch. Some features may be omitted.\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 1.32s\n",
            "I1016 19:05:50.465670 132297464188928 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 1.32s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 1.32s\n",
            "I1016 19:05:51.788519 132297464188928 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 1.32s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.75s\n",
            "I1016 19:05:52.536680 132297464188928 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.75s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 1.04s\n",
            "I1016 19:05:53.576181 132297464188928 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 1.04s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 5.69s\n",
            "I1016 19:05:59.264672 132297464188928 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 5.69s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_experimental_model\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.01s\n",
            "I1016 19:05:59.278235 132297464188928 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.01s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_experimental_model\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.07s\n",
            "I1016 19:05:59.353922 132297464188928 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.07s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.04s\n",
            "I1016 19:05:59.398545 132297464188928 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.04s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.04s\n",
            "I1016 19:05:59.444690 132297464188928 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.04s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.26s\n",
            "I1016 19:05:59.707497 132297464188928 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.26s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.27s\n",
            "I1016 19:05:59.978292 132297464188928 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.27s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.29s\n",
            "I1016 19:06:00.266331 132297464188928 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.29s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.29s\n",
            "I1016 19:06:00.554932 132297464188928 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.29s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.28s\n",
            "I1016 19:06:00.833853 132297464188928 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.28s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.08s\n",
            "I1016 19:06:00.915221 132297464188928 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.08s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
            "I1016 19:06:01.223025 132297464188928 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b0\n",
            "I1016 19:06:01.223289 132297464188928 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 64\n",
            "I1016 19:06:01.223388 132297464188928 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 3\n",
            "I1016 19:06:01.227723 132297464188928 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I1016 19:06:01.271113 132297464188928 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I1016 19:06:01.271337 132297464188928 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I1016 19:06:01.401172 132297464188928 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I1016 19:06:01.401549 132297464188928 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I1016 19:06:01.740220 132297464188928 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I1016 19:06:01.740474 132297464188928 efficientnet_model.py:143] round_filter input=40 output=40\n",
            "I1016 19:06:02.055856 132297464188928 efficientnet_model.py:143] round_filter input=40 output=40\n",
            "I1016 19:06:02.056171 132297464188928 efficientnet_model.py:143] round_filter input=80 output=80\n",
            "I1016 19:06:02.478772 132297464188928 efficientnet_model.py:143] round_filter input=80 output=80\n",
            "I1016 19:06:02.479011 132297464188928 efficientnet_model.py:143] round_filter input=112 output=112\n",
            "I1016 19:06:02.960881 132297464188928 efficientnet_model.py:143] round_filter input=112 output=112\n",
            "I1016 19:06:02.962411 132297464188928 efficientnet_model.py:143] round_filter input=192 output=192\n",
            "I1016 19:06:03.823563 132297464188928 efficientnet_model.py:143] round_filter input=192 output=192\n",
            "I1016 19:06:03.823811 132297464188928 efficientnet_model.py:143] round_filter input=320 output=320\n",
            "I1016 19:06:04.063039 132297464188928 efficientnet_model.py:143] round_filter input=1280 output=1280\n",
            "I1016 19:06:04.227191 132297464188928 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1016 19:06:04.368873 132297464188928 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b1\n",
            "I1016 19:06:04.369204 132297464188928 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 88\n",
            "I1016 19:06:04.369328 132297464188928 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 4\n",
            "I1016 19:06:04.373667 132297464188928 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I1016 19:06:04.415619 132297464188928 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I1016 19:06:04.415966 132297464188928 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I1016 19:06:04.741386 132297464188928 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I1016 19:06:04.741702 132297464188928 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I1016 19:06:05.863854 132297464188928 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I1016 19:06:05.864187 132297464188928 efficientnet_model.py:143] round_filter input=40 output=40\n",
            "I1016 19:06:06.496744 132297464188928 efficientnet_model.py:143] round_filter input=40 output=40\n",
            "I1016 19:06:06.497110 132297464188928 efficientnet_model.py:143] round_filter input=80 output=80\n",
            "I1016 19:06:07.414008 132297464188928 efficientnet_model.py:143] round_filter input=80 output=80\n",
            "I1016 19:06:07.414345 132297464188928 efficientnet_model.py:143] round_filter input=112 output=112\n",
            "I1016 19:06:08.329981 132297464188928 efficientnet_model.py:143] round_filter input=112 output=112\n",
            "I1016 19:06:08.330301 132297464188928 efficientnet_model.py:143] round_filter input=192 output=192\n",
            "I1016 19:06:09.511691 132297464188928 efficientnet_model.py:143] round_filter input=192 output=192\n",
            "I1016 19:06:09.512037 132297464188928 efficientnet_model.py:143] round_filter input=320 output=320\n",
            "I1016 19:06:10.079870 132297464188928 efficientnet_model.py:143] round_filter input=1280 output=1280\n",
            "I1016 19:06:10.203842 132297464188928 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1016 19:06:10.364996 132297464188928 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b2\n",
            "I1016 19:06:10.365323 132297464188928 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 112\n",
            "I1016 19:06:10.365464 132297464188928 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 5\n",
            "I1016 19:06:10.369944 132297464188928 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I1016 19:06:10.413760 132297464188928 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I1016 19:06:10.414109 132297464188928 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I1016 19:06:10.760574 132297464188928 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I1016 19:06:10.761052 132297464188928 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I1016 19:06:11.544092 132297464188928 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I1016 19:06:11.544433 132297464188928 efficientnet_model.py:143] round_filter input=40 output=48\n",
            "I1016 19:06:12.258139 132297464188928 efficientnet_model.py:143] round_filter input=40 output=48\n",
            "I1016 19:06:12.258462 132297464188928 efficientnet_model.py:143] round_filter input=80 output=88\n",
            "I1016 19:06:13.431883 132297464188928 efficientnet_model.py:143] round_filter input=80 output=88\n",
            "I1016 19:06:13.432221 132297464188928 efficientnet_model.py:143] round_filter input=112 output=120\n",
            "I1016 19:06:14.673138 132297464188928 efficientnet_model.py:143] round_filter input=112 output=120\n",
            "I1016 19:06:14.673439 132297464188928 efficientnet_model.py:143] round_filter input=192 output=208\n",
            "I1016 19:06:16.274673 132297464188928 efficientnet_model.py:143] round_filter input=192 output=208\n",
            "I1016 19:06:16.274934 132297464188928 efficientnet_model.py:143] round_filter input=320 output=352\n",
            "I1016 19:06:16.865873 132297464188928 efficientnet_model.py:143] round_filter input=1280 output=1408\n",
            "I1016 19:06:17.026738 132297464188928 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.1, depth_coefficient=1.2, resolution=260, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1016 19:06:17.163459 132297464188928 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b3\n",
            "I1016 19:06:17.163718 132297464188928 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 160\n",
            "I1016 19:06:17.163811 132297464188928 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 6\n",
            "I1016 19:06:17.167876 132297464188928 efficientnet_model.py:143] round_filter input=32 output=40\n",
            "I1016 19:06:17.216402 132297464188928 efficientnet_model.py:143] round_filter input=32 output=40\n",
            "I1016 19:06:17.216730 132297464188928 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I1016 19:06:17.579291 132297464188928 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I1016 19:06:17.579641 132297464188928 efficientnet_model.py:143] round_filter input=24 output=32\n",
            "I1016 19:06:18.309520 132297464188928 efficientnet_model.py:143] round_filter input=24 output=32\n",
            "I1016 19:06:18.309845 132297464188928 efficientnet_model.py:143] round_filter input=40 output=48\n",
            "I1016 19:06:19.039406 132297464188928 efficientnet_model.py:143] round_filter input=40 output=48\n",
            "I1016 19:06:19.039748 132297464188928 efficientnet_model.py:143] round_filter input=80 output=96\n",
            "I1016 19:06:20.193379 132297464188928 efficientnet_model.py:143] round_filter input=80 output=96\n",
            "I1016 19:06:20.193713 132297464188928 efficientnet_model.py:143] round_filter input=112 output=136\n",
            "I1016 19:06:21.419085 132297464188928 efficientnet_model.py:143] round_filter input=112 output=136\n",
            "I1016 19:06:21.419502 132297464188928 efficientnet_model.py:143] round_filter input=192 output=232\n",
            "I1016 19:06:23.107554 132297464188928 efficientnet_model.py:143] round_filter input=192 output=232\n",
            "I1016 19:06:23.107876 132297464188928 efficientnet_model.py:143] round_filter input=320 output=384\n",
            "I1016 19:06:23.710586 132297464188928 efficientnet_model.py:143] round_filter input=1280 output=1536\n",
            "I1016 19:06:23.867556 132297464188928 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.2, depth_coefficient=1.4, resolution=300, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1016 19:06:24.028862 132297464188928 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b4\n",
            "I1016 19:06:24.029144 132297464188928 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 224\n",
            "I1016 19:06:24.029260 132297464188928 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 7\n",
            "I1016 19:06:24.033111 132297464188928 efficientnet_model.py:143] round_filter input=32 output=48\n",
            "I1016 19:06:24.072705 132297464188928 efficientnet_model.py:143] round_filter input=32 output=48\n",
            "I1016 19:06:24.072983 132297464188928 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I1016 19:06:24.380231 132297464188928 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I1016 19:06:24.380483 132297464188928 efficientnet_model.py:143] round_filter input=24 output=32\n",
            "I1016 19:06:25.464542 132297464188928 efficientnet_model.py:143] round_filter input=24 output=32\n",
            "I1016 19:06:25.464826 132297464188928 efficientnet_model.py:143] round_filter input=40 output=56\n",
            "I1016 19:06:26.088445 132297464188928 efficientnet_model.py:143] round_filter input=40 output=56\n",
            "I1016 19:06:26.088720 132297464188928 efficientnet_model.py:143] round_filter input=80 output=112\n",
            "I1016 19:06:27.009176 132297464188928 efficientnet_model.py:143] round_filter input=80 output=112\n",
            "I1016 19:06:27.009436 132297464188928 efficientnet_model.py:143] round_filter input=112 output=160\n",
            "I1016 19:06:27.980867 132297464188928 efficientnet_model.py:143] round_filter input=112 output=160\n",
            "I1016 19:06:27.981151 132297464188928 efficientnet_model.py:143] round_filter input=192 output=272\n",
            "I1016 19:06:29.417105 132297464188928 efficientnet_model.py:143] round_filter input=192 output=272\n",
            "I1016 19:06:29.417373 132297464188928 efficientnet_model.py:143] round_filter input=320 output=448\n",
            "I1016 19:06:29.885574 132297464188928 efficientnet_model.py:143] round_filter input=1280 output=1792\n",
            "I1016 19:06:30.009054 132297464188928 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.4, depth_coefficient=1.8, resolution=380, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1016 19:06:30.123146 132297464188928 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b5\n",
            "I1016 19:06:30.123397 132297464188928 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 288\n",
            "I1016 19:06:30.123467 132297464188928 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 7\n",
            "I1016 19:06:30.126101 132297464188928 efficientnet_model.py:143] round_filter input=32 output=48\n",
            "I1016 19:06:30.153336 132297464188928 efficientnet_model.py:143] round_filter input=32 output=48\n",
            "I1016 19:06:30.153589 132297464188928 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I1016 19:06:30.521272 132297464188928 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I1016 19:06:30.521588 132297464188928 efficientnet_model.py:143] round_filter input=24 output=40\n",
            "I1016 19:06:31.227289 132297464188928 efficientnet_model.py:143] round_filter input=24 output=40\n",
            "I1016 19:06:31.227550 132297464188928 efficientnet_model.py:143] round_filter input=40 output=64\n",
            "I1016 19:06:31.992795 132297464188928 efficientnet_model.py:143] round_filter input=40 output=64\n",
            "I1016 19:06:31.993114 132297464188928 efficientnet_model.py:143] round_filter input=80 output=128\n",
            "I1016 19:06:33.072244 132297464188928 efficientnet_model.py:143] round_filter input=80 output=128\n",
            "I1016 19:06:33.072510 132297464188928 efficientnet_model.py:143] round_filter input=112 output=176\n",
            "I1016 19:06:34.215385 132297464188928 efficientnet_model.py:143] round_filter input=112 output=176\n",
            "I1016 19:06:34.215651 132297464188928 efficientnet_model.py:143] round_filter input=192 output=304\n",
            "I1016 19:06:36.241836 132297464188928 efficientnet_model.py:143] round_filter input=192 output=304\n",
            "I1016 19:06:36.242188 132297464188928 efficientnet_model.py:143] round_filter input=320 output=512\n",
            "I1016 19:06:37.857369 132297464188928 efficientnet_model.py:143] round_filter input=1280 output=2048\n",
            "I1016 19:06:38.047306 132297464188928 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.6, depth_coefficient=2.2, resolution=456, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1016 19:06:38.741641 132297464188928 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b6\n",
            "I1016 19:06:38.742005 132297464188928 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 384\n",
            "I1016 19:06:38.742123 132297464188928 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 8\n",
            "I1016 19:06:38.746672 132297464188928 efficientnet_model.py:143] round_filter input=32 output=56\n",
            "I1016 19:06:38.791382 132297464188928 efficientnet_model.py:143] round_filter input=32 output=56\n",
            "I1016 19:06:38.791689 132297464188928 efficientnet_model.py:143] round_filter input=16 output=32\n",
            "I1016 19:06:39.312722 132297464188928 efficientnet_model.py:143] round_filter input=16 output=32\n",
            "I1016 19:06:39.313043 132297464188928 efficientnet_model.py:143] round_filter input=24 output=40\n",
            "I1016 19:06:40.757771 132297464188928 efficientnet_model.py:143] round_filter input=24 output=40\n",
            "I1016 19:06:40.758326 132297464188928 efficientnet_model.py:143] round_filter input=40 output=72\n",
            "I1016 19:06:42.917640 132297464188928 efficientnet_model.py:143] round_filter input=40 output=72\n",
            "I1016 19:06:42.917997 132297464188928 efficientnet_model.py:143] round_filter input=80 output=144\n",
            "I1016 19:06:44.897510 132297464188928 efficientnet_model.py:143] round_filter input=80 output=144\n",
            "I1016 19:06:44.897869 132297464188928 efficientnet_model.py:143] round_filter input=112 output=200\n",
            "I1016 19:06:47.026607 132297464188928 efficientnet_model.py:143] round_filter input=112 output=200\n",
            "I1016 19:06:47.026876 132297464188928 efficientnet_model.py:143] round_filter input=192 output=344\n",
            "I1016 19:06:49.066328 132297464188928 efficientnet_model.py:143] round_filter input=192 output=344\n",
            "I1016 19:06:49.066583 132297464188928 efficientnet_model.py:143] round_filter input=320 output=576\n",
            "I1016 19:06:49.869264 132297464188928 efficientnet_model.py:143] round_filter input=1280 output=2304\n",
            "I1016 19:06:49.997843 132297464188928 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.8, depth_coefficient=2.6, resolution=528, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1016 19:06:50.167530 132297464188928 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b7\n",
            "I1016 19:06:50.168197 132297464188928 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 384\n",
            "I1016 19:06:50.168404 132297464188928 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 8\n",
            "I1016 19:06:50.174572 132297464188928 efficientnet_model.py:143] round_filter input=32 output=64\n",
            "I1016 19:06:50.208838 132297464188928 efficientnet_model.py:143] round_filter input=32 output=64\n",
            "I1016 19:06:50.209131 132297464188928 efficientnet_model.py:143] round_filter input=16 output=32\n",
            "I1016 19:06:50.690382 132297464188928 efficientnet_model.py:143] round_filter input=16 output=32\n",
            "I1016 19:06:50.690658 132297464188928 efficientnet_model.py:143] round_filter input=24 output=48\n",
            "I1016 19:06:51.653412 132297464188928 efficientnet_model.py:143] round_filter input=24 output=48\n",
            "I1016 19:06:51.653662 132297464188928 efficientnet_model.py:143] round_filter input=40 output=80\n",
            "I1016 19:06:52.583168 132297464188928 efficientnet_model.py:143] round_filter input=40 output=80\n",
            "I1016 19:06:52.583405 132297464188928 efficientnet_model.py:143] round_filter input=80 output=160\n",
            "I1016 19:06:54.082542 132297464188928 efficientnet_model.py:143] round_filter input=80 output=160\n",
            "I1016 19:06:54.082790 132297464188928 efficientnet_model.py:143] round_filter input=112 output=224\n",
            "I1016 19:06:55.998672 132297464188928 efficientnet_model.py:143] round_filter input=112 output=224\n",
            "I1016 19:06:55.998953 132297464188928 efficientnet_model.py:143] round_filter input=192 output=384\n",
            "I1016 19:06:59.419493 132297464188928 efficientnet_model.py:143] round_filter input=192 output=384\n",
            "I1016 19:06:59.419786 132297464188928 efficientnet_model.py:143] round_filter input=320 output=640\n",
            "I1016 19:07:01.302724 132297464188928 efficientnet_model.py:143] round_filter input=1280 output=2560\n",
            "I1016 19:07:01.555551 132297464188928 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=2.0, depth_coefficient=3.1, resolution=600, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 61.73s\n",
            "I1016 19:07:02.645669 132297464188928 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 61.73s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.01s\n",
            "I1016 19:07:02.719352 132297464188928 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.01s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
            "I1016 19:07:02.723341 132297464188928 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
            "I1016 19:07:02.724500 132297464188928 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
            "I1016 19:07:02.727788 132297464188928 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
            "[ RUN      ] ModelBuilderTF2Test.test_session\n",
            "[  SKIPPED ] ModelBuilderTF2Test.test_session\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
            "I1016 19:07:02.730618 132297464188928 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
            "I1016 19:07:02.731482 132297464188928 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
            "I1016 19:07:02.733500 132297464188928 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
            "----------------------------------------------------------------------\n",
            "Ran 24 tests in 73.589s\n",
            "\n",
            "OK (skipped=1)\n"
          ]
        }
      ],
      "source": [
        "VERIFICATION_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'builders', 'model_builder_tf2_test.py')\n",
        "\n",
        "# verify Installation\n",
        "!python {VERIFICATION_SCRIPT}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQ_LACkBdG7K"
      },
      "outputs": [],
      "source": [
        "# solve for ModuleNotFoundError: No module named 'tensorflow'\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9u0pvR1RdG7K",
        "outputId": "68f69717-a0c7-42f3-9036-58ff7dceae73"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nTypeError: Descriptors cannot not be created directly.\\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\\n 1. Downgrade the protobuf package to 3.20.x or lower.\\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\\n'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "solve:\n",
        "TypeError: Descriptors cannot not be created directly.\n",
        "If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n",
        "If you cannot immediately regenerate your protos, some other possible workarounds are:\n",
        " 1. Downgrade the protobuf package to 3.20.x or lower.\n",
        " 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n",
        "'''\n",
        "\n",
        "!pip uninstall protobuf matplotlib -y\n",
        "!pip install protobuf matplotlib==3.2\n",
        "!pip install pyyaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fd0QP04AdG7K"
      },
      "outputs": [],
      "source": [
        "# incase there is a tensorflow object detection api error then follow this link https://github.com/tensorflow/models/issues/11085\n",
        "!pip install matplotlib\n",
        "!pip install pandas\n",
        "!pip install scipy\n",
        "!pip install apache-beam avro-python3 contextlib2 Cython lvis pycocotools\n",
        "!pip install protobuf\n",
        "!pip install pyyaml\n",
        "!pip install tensorflow-io\n",
        "!pip install portalocker tabulate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDtQ1EfCdG7K"
      },
      "source": [
        "If the text in a Markdown cell appears too long and you want to hide or shorten it without deleting the content, you can use a combination of techniques:\n",
        "\n",
        "1. **Use a Collapsible Section:** You can create a collapsible section in your Markdown cell by using HTML and JavaScript. This allows you to hide or show content as needed. Here's an example:\n",
        "\n",
        "   ```markdown\n",
        "   <details>\n",
        "   <summary>Click to expand</summary>\n",
        "\n",
        "   Your long text goes here.\n",
        "\n",
        "   </details>\n",
        "   ```\n",
        "\n",
        "   In this example, the long text is initially hidden, and you can click \"Click to expand\" to reveal it.\n",
        "\n",
        "To access your command history, you can use the history command:\n",
        "\n",
        "To load and access command history in a Bash shell on a Windows system (assuming you are using a Bash emulation layer like Git Bash or WSL - Windows Subsystem for Linux), you can use a combination of built-in commands and environment variables. Here's how to do it:\n",
        "\n",
        "1. Open your Bash shell, such as Git Bash or WSL.\n",
        "\n",
        "2. To access your command history, you can use the `history` command:\n",
        "\n",
        "   ```\n",
        "   history\n",
        "   ```\n",
        "\n",
        "   This will display a list of previously executed commands along with line numbers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF0hAw0NdG7L"
      },
      "source": [
        "a list of all command prompts i used while solving issues.\n",
        "<details>\n",
        "<summary>Click to expand</summary>\n",
        "\n",
        "1. `pip list`\n",
        "2. `pip install tensorflow`\n",
        "3. `pip uninstall protobuf matplotlib -y`\n",
        "4. `pip install protobuf matplotlib==3.2`\n",
        "5. `pip install matplotlib`\n",
        "6. `pip install pandas`\n",
        "7. `pip install scipy`\n",
        "8. `pip install object-detection==0.1 apache-beam avro-python3 contextlib2 Cython lvis pycocotools`\n",
        "9.  `pip install matplotlib`\n",
        "10. `pip install protobuf`\n",
        "11. `pip install pyyaml`\n",
        "12. `pip uninstall protobuff`\n",
        "13. `pip install protobuf==3.20.3`\n",
        "14. `pip install tensorflow-io`\n",
        "15. `pip install object-detection==0.1 apache-beam avro-python3 contextlib2 Cython lvis pycocotools tf-models-official>=2.5.1`\n",
        "16. `pip install apache-beam avro-python3 contextlib2 Cython lvis pycocotools tf-models-official>=2.5.1`\n",
        "17. `pip install apache-beam avro-python3 contextlib2 Cython lvis pycocotools`\n",
        "18. `pip install tf-models-official`\n",
        "19. `pip install portalocker`\n",
        "20. `pip install tabulate`\n",
        "21. `pip install tf-models-official`\n",
        "22. `pip install tf-models-official`\n",
        "23. `pip install --upgrade setuptools`\n",
        "24. `pip install tf-models-official`\n",
        "25. `pip install tf-models-official`\n",
        "26. `cd tensorflow`\n",
        "27. `git clone https://github.com/tensorflow/models models`\n",
        "28. `cd models`\n",
        "29. `git clone https://github.com/tensorflow/models`\n",
        "30. `git clone https://github.com/tensorflow/models`\n",
        "31. `cd ..`\n",
        "32. `cd ..`\n",
        "33. `pip uninstall Cython -y`\n",
        "34. `cd tensorflow/models`\n",
        "35. `cd research`\n",
        "36. `protoc object_detection/protos/*.proto --python_out= .`\n",
        "37. `cp object_detection/packages/tf2/setup.py .`\n",
        "38. `python -m pip install .`\n",
        "39. `protoc object_detection/protos/*.proto --python_out= .`\n",
        "40. `protoc object_detection/protos/*.proto --python_out= .`\n",
        "41. `cd ..`\n",
        "42. `cd ..`\n",
        "43. `cd ..`\n",
        "44. `cd models/research`\n",
        "45. `protoc object_detection/protos/*.proto --python_out= .`\n",
        "46. `cp object_detection/packages/tf2/setup.py .`\n",
        "\n",
        "</details>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "YcyZKQ3fdG7L"
      },
      "outputs": [],
      "source": [
        "import object_detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ngognpTzdG7L",
        "outputId": "e502a7be-8387-4e5e-e394-cb68671a7601",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-16 19:07:24--  http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 173.194.217.207, 173.194.218.207, 108.177.12.207, ...\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|173.194.217.207|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20518283 (20M) [application/x-tar]\n",
            "Saving to: ‘ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz’\n",
            "\n",
            "ssd_mobilenet_v2_fp 100%[===================>]  19.57M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-10-16 19:07:25 (150 MB/s) - ‘ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz’ saved [20518283/20518283]\n",
            "\n",
            "ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/\n",
            "ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/checkpoint/\n",
            "ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/checkpoint/ckpt-0.data-00000-of-00001\n",
            "ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/checkpoint/checkpoint\n",
            "ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/checkpoint/ckpt-0.index\n",
            "ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/pipeline.config\n",
            "ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/saved_model/\n",
            "ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/saved_model/saved_model.pb\n",
            "ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/saved_model/variables/\n",
            "ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/saved_model/variables/variables.data-00000-of-00001\n",
            "ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/saved_model/variables/variables.index\n"
          ]
        }
      ],
      "source": [
        "if os.name =='posix':\n",
        "    !wget {PRETRAINED_MODEL_URL}\n",
        "    !mv {PRETRAINED_MODEL_NAME+'.tar.gz'} {paths['PRETRAINED_MODEL_PATH']}\n",
        "    !cd {paths['PRETRAINED_MODEL_PATH']} && tar -zxvf {PRETRAINED_MODEL_NAME+'.tar.gz'}\n",
        "if os.name == 'nt':\n",
        "    wget.download(PRETRAINED_MODEL_URL)\n",
        "    !move {PRETRAINED_MODEL_NAME+'.tar.gz'} {paths['PRETRAINED_MODEL_PATH']}\n",
        "    !cd {paths['PRETRAINED_MODEL_PATH']} && tar -zxvf {PRETRAINED_MODEL_NAME+'.tar.gz'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "X0HcyiildG7L",
        "outputId": "75f9008d-56f4-4d4d-c681-769e2da40821",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensorflow/workspace/pretrained_models\n"
          ]
        }
      ],
      "source": [
        "print(paths['PRETRAINED_MODEL_PATH'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUN_SCNhdG7M"
      },
      "source": [
        "If your network connection is not reliable or is running slowly, you can download the model directly from [this link](http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8.tar.gz). Once downloaded, you can unzip the file using a tool like WinRAR.\n",
        "\n",
        "Ensure that you extract the model to the following location: `tensorflow/workspace/pretrained_models`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OanNefavdG7M"
      },
      "source": [
        "## 3.3 **Create a Label Map**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjqMb1uvdG7M"
      },
      "source": [
        "our labels consist of numbers from 0 to 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "rt8UDjMMdG7M"
      },
      "outputs": [],
      "source": [
        "labels = [\n",
        "    {'name':'0', 'id':1},\n",
        "    {'name':'1', 'id':2},\n",
        "    {'name':'2', 'id':3},\n",
        "    {'name':'3', 'id':4},\n",
        "    {'name':'4', 'id':5},\n",
        "    {'name':'5', 'id':6},\n",
        "    {'name':'6', 'id':7},\n",
        "    {'name':'7', 'id':8},\n",
        "    {'name':'8', 'id':9},\n",
        "    {'name':'9', 'id':10}\n",
        "]\n",
        "\n",
        "# here, a label map file is being created where each label is defined with its name and ID\n",
        "with open(files['LABELMAP'], 'w') as f:\n",
        "    for label in labels:\n",
        "        f.write('item { \\n')\n",
        "        f.write('\\tname:\\'{}\\'\\n'.format(label['name']))\n",
        "        f.write('\\tid:{}\\n'.format(label['id']))\n",
        "        f.write('}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "0v_kVx27dG7N",
        "outputId": "8a51345c-ae1f-412e-805e-c531832e50db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensorflow/workspace/annotations/label_map.pbtxt\n"
          ]
        }
      ],
      "source": [
        "print(files['LABELMAP'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3402VYLdG7N"
      },
      "source": [
        "## **3.4 Create TF records**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "PPT4kOdMdG7N",
        "outputId": "165a84bf-18ca-4d31-e1d2-050c5f11836b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'tensorflow/scripts'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 3 (delta 0), reused 1 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (3/3), done.\n"
          ]
        }
      ],
      "source": [
        "# clone nicks tf python script\n",
        "if not os.path.exists(files['TF_RECORD_SCRIPT']):\n",
        "    !git clone https://github.com/nicknochnack/GenerateTFRecord {paths['SCRIPTS_PATH']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "HzEHzfAFdG7N",
        "outputId": "7c80f2ba-8cad-4f80-8fa9-e6db92ad263d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (2023.3.post1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "dZE1GAFldG7N",
        "outputId": "9a9aef25-571f-4dce-e8a7-75bab2a3489a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully created the TFRecord file: tensorflow/workspace/annotations/train.record\n",
            "Successfully created the TFRecord file: tensorflow/workspace/annotations/test.record\n"
          ]
        }
      ],
      "source": [
        "!python {files['TF_RECORD_SCRIPT']} -x {os.path.join(paths['IMAGE_PATH'], 'train')} -l {files['LABELMAP']} -o {os.path.join(paths['ANNOTATION_PATH'], 'train.record')}\n",
        "!python {files['TF_RECORD_SCRIPT']} -x {os.path.join(paths['IMAGE_PATH'], 'test')} -l {files['LABELMAP']} -o {os.path.join(paths['ANNOTATION_PATH'], 'test.record')}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "8QIWkeQ2dG7O",
        "outputId": "ef6ce306-39d0-4cc1-a8a6-4cd66405ebb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensorflow/workspace/pretrained_models\n"
          ]
        }
      ],
      "source": [
        "print(paths['PRETRAINED_MODEL_PATH'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW5_RYeYdG7O"
      },
      "source": [
        "what the code does is basically running the python script inside of tf_record_script, opens up the image source file in train/test, opens up the labelmap folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HGWYjRFdG7O"
      },
      "source": [
        "## 3.4 **copy model config to training folder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "WV5IfuJAdG7O"
      },
      "outputs": [],
      "source": [
        "if os.name == 'posix':\n",
        "    !cp {os.path.join(paths['PRETRAINED_MODEL_PATH'], PRETRAINED_MODEL_NAME, 'pipeline.config')} {os.path.join(paths['CHECKPOINT_PATH'])}\n",
        "if os.name == 'nt':\n",
        "    !copy {os.path.join(paths['PRETRAINED_MODEL_PATH'], PRETRAINED_MODEL_NAME, 'pipeline.config')} {os.path.join(paths['CHECKPOINT_PATH'])}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6_SSnQUdG7P"
      },
      "source": [
        "## 3.5 **Update Config to Training Folder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "LpY_Mh3YdG7P"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from object_detection.utils import config_util #manipulate config files\n",
        "from object_detection.protos import pipeline_pb2 #define pipeline configs\n",
        "from google.protobuf import text_format #covert config files into protobuf n human text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "nYumn6VbdG7P"
      },
      "outputs": [],
      "source": [
        "config = config_util.get_configs_from_pipeline_file(files['PIPELINE_CONFIG'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "GGbAv6m7dG7P",
        "outputId": "6a4c405d-9750-4979-c0e6-345ea69e5c92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model': ssd {\n",
              "   num_classes: 90\n",
              "   image_resizer {\n",
              "     fixed_shape_resizer {\n",
              "       height: 640\n",
              "       width: 640\n",
              "     }\n",
              "   }\n",
              "   feature_extractor {\n",
              "     type: \"ssd_mobilenet_v2_fpn_keras\"\n",
              "     depth_multiplier: 1.0\n",
              "     min_depth: 16\n",
              "     conv_hyperparams {\n",
              "       regularizer {\n",
              "         l2_regularizer {\n",
              "           weight: 3.9999998989515007e-05\n",
              "         }\n",
              "       }\n",
              "       initializer {\n",
              "         random_normal_initializer {\n",
              "           mean: 0.0\n",
              "           stddev: 0.009999999776482582\n",
              "         }\n",
              "       }\n",
              "       activation: RELU_6\n",
              "       batch_norm {\n",
              "         decay: 0.996999979019165\n",
              "         scale: true\n",
              "         epsilon: 0.0010000000474974513\n",
              "       }\n",
              "     }\n",
              "     use_depthwise: true\n",
              "     override_base_feature_extractor_hyperparams: true\n",
              "     fpn {\n",
              "       min_level: 3\n",
              "       max_level: 7\n",
              "       additional_layer_depth: 128\n",
              "     }\n",
              "   }\n",
              "   box_coder {\n",
              "     faster_rcnn_box_coder {\n",
              "       y_scale: 10.0\n",
              "       x_scale: 10.0\n",
              "       height_scale: 5.0\n",
              "       width_scale: 5.0\n",
              "     }\n",
              "   }\n",
              "   matcher {\n",
              "     argmax_matcher {\n",
              "       matched_threshold: 0.5\n",
              "       unmatched_threshold: 0.5\n",
              "       ignore_thresholds: false\n",
              "       negatives_lower_than_unmatched: true\n",
              "       force_match_for_each_row: true\n",
              "       use_matmul_gather: true\n",
              "     }\n",
              "   }\n",
              "   similarity_calculator {\n",
              "     iou_similarity {\n",
              "     }\n",
              "   }\n",
              "   box_predictor {\n",
              "     weight_shared_convolutional_box_predictor {\n",
              "       conv_hyperparams {\n",
              "         regularizer {\n",
              "           l2_regularizer {\n",
              "             weight: 3.9999998989515007e-05\n",
              "           }\n",
              "         }\n",
              "         initializer {\n",
              "           random_normal_initializer {\n",
              "             mean: 0.0\n",
              "             stddev: 0.009999999776482582\n",
              "           }\n",
              "         }\n",
              "         activation: RELU_6\n",
              "         batch_norm {\n",
              "           decay: 0.996999979019165\n",
              "           scale: true\n",
              "           epsilon: 0.0010000000474974513\n",
              "         }\n",
              "       }\n",
              "       depth: 128\n",
              "       num_layers_before_predictor: 4\n",
              "       kernel_size: 3\n",
              "       class_prediction_bias_init: -4.599999904632568\n",
              "       share_prediction_tower: true\n",
              "       use_depthwise: true\n",
              "     }\n",
              "   }\n",
              "   anchor_generator {\n",
              "     multiscale_anchor_generator {\n",
              "       min_level: 3\n",
              "       max_level: 7\n",
              "       anchor_scale: 4.0\n",
              "       aspect_ratios: 1.0\n",
              "       aspect_ratios: 2.0\n",
              "       aspect_ratios: 0.5\n",
              "       scales_per_octave: 2\n",
              "     }\n",
              "   }\n",
              "   post_processing {\n",
              "     batch_non_max_suppression {\n",
              "       score_threshold: 9.99999993922529e-09\n",
              "       iou_threshold: 0.6000000238418579\n",
              "       max_detections_per_class: 100\n",
              "       max_total_detections: 100\n",
              "       use_static_shapes: false\n",
              "     }\n",
              "     score_converter: SIGMOID\n",
              "   }\n",
              "   normalize_loss_by_num_matches: true\n",
              "   loss {\n",
              "     localization_loss {\n",
              "       weighted_smooth_l1 {\n",
              "       }\n",
              "     }\n",
              "     classification_loss {\n",
              "       weighted_sigmoid_focal {\n",
              "         gamma: 2.0\n",
              "         alpha: 0.25\n",
              "       }\n",
              "     }\n",
              "     classification_weight: 1.0\n",
              "     localization_weight: 1.0\n",
              "   }\n",
              "   encode_background_as_zeros: true\n",
              "   normalize_loc_loss_by_codesize: true\n",
              "   inplace_batchnorm_update: true\n",
              "   freeze_batchnorm: false\n",
              " },\n",
              " 'train_config': batch_size: 128\n",
              " data_augmentation_options {\n",
              "   random_horizontal_flip {\n",
              "   }\n",
              " }\n",
              " data_augmentation_options {\n",
              "   random_crop_image {\n",
              "     min_object_covered: 0.0\n",
              "     min_aspect_ratio: 0.75\n",
              "     max_aspect_ratio: 3.0\n",
              "     min_area: 0.75\n",
              "     max_area: 1.0\n",
              "     overlap_thresh: 0.0\n",
              "   }\n",
              " }\n",
              " sync_replicas: true\n",
              " optimizer {\n",
              "   momentum_optimizer {\n",
              "     learning_rate {\n",
              "       cosine_decay_learning_rate {\n",
              "         learning_rate_base: 0.07999999821186066\n",
              "         total_steps: 50000\n",
              "         warmup_learning_rate: 0.026666000485420227\n",
              "         warmup_steps: 1000\n",
              "       }\n",
              "     }\n",
              "     momentum_optimizer_value: 0.8999999761581421\n",
              "   }\n",
              "   use_moving_average: false\n",
              " }\n",
              " fine_tune_checkpoint: \"PATH_TO_BE_CONFIGURED\"\n",
              " num_steps: 50000\n",
              " startup_delay_steps: 0.0\n",
              " replicas_to_aggregate: 8\n",
              " max_number_of_boxes: 100\n",
              " unpad_groundtruth_tensors: false\n",
              " fine_tune_checkpoint_type: \"classification\"\n",
              " fine_tune_checkpoint_version: V2,\n",
              " 'train_input_config': label_map_path: \"PATH_TO_BE_CONFIGURED\"\n",
              " tf_record_input_reader {\n",
              "   input_path: \"PATH_TO_BE_CONFIGURED\"\n",
              " },\n",
              " 'eval_config': metrics_set: \"coco_detection_metrics\"\n",
              " use_moving_averages: false,\n",
              " 'eval_input_configs': [label_map_path: \"PATH_TO_BE_CONFIGURED\"\n",
              " shuffle: false\n",
              " num_epochs: 1\n",
              " tf_record_input_reader {\n",
              "   input_path: \"PATH_TO_BE_CONFIGURED\"\n",
              " }\n",
              " ],\n",
              " 'eval_input_config': label_map_path: \"PATH_TO_BE_CONFIGURED\"\n",
              " shuffle: false\n",
              " num_epochs: 1\n",
              " tf_record_input_reader {\n",
              "   input_path: \"PATH_TO_BE_CONFIGURED\"\n",
              " }}"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ],
      "source": [
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "xFTd6DhidG7P"
      },
      "outputs": [],
      "source": [
        "pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
        "with tf.io.gfile.GFile(files['PIPELINE_CONFIG'], 'r') as f:\n",
        "    proto_str = f.read()\n",
        "    text_format.Merge(proto_str, pipeline_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "SWUosohldG7Q"
      },
      "outputs": [],
      "source": [
        "pipeline_config.model.ssd.num_classes = len(labels)\n",
        "pipeline_config.train_config.batch_size = 4\n",
        "pipeline_config.train_config.fine_tune_checkpoint = os.path.join(paths['PRETRAINED_MODEL_PATH'], PRETRAINED_MODEL_NAME, 'checkpoint', 'ckpt-0')\n",
        "pipeline_config.train_config.fine_tune_checkpoint_type = \"detection\"\n",
        "pipeline_config.train_input_reader.label_map_path= files['LABELMAP']\n",
        "pipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [os.path.join(paths['ANNOTATION_PATH'], 'train.record')]\n",
        "pipeline_config.eval_input_reader[0].label_map_path = files['LABELMAP']\n",
        "pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [os.path.join(paths['ANNOTATION_PATH'], 'test.record')]\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Vk2K52JLdG7Q"
      },
      "outputs": [],
      "source": [
        "config_text = text_format.MessageToString(pipeline_config)\n",
        "with tf.io.gfile.GFile(files['PIPELINE_CONFIG'], 'wb') as f:\n",
        "    f.write(config_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqKzti18dG7Q"
      },
      "source": [
        "## 3.5 *Train The Model*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "aAg7nIkkdG7Q"
      },
      "outputs": [],
      "source": [
        "TRAINING_SCRIPT = os.path.join(paths[\"APIMODEL_PATH\"], 'research', 'object_detection', 'model_main_tf2.py')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "4QyUSvuMdG7Q"
      },
      "outputs": [],
      "source": [
        "command = \"python {} --model_dir={} --pipeline_config_path={} --num_train_steps=1000\".format(TRAINING_SCRIPT, paths['CHECKPOINT_PATH'],files['PIPELINE_CONFIG'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "eR5kxqIMdG7R",
        "outputId": "03a6e86f-5b9b-4799-de89-5ffd620c93f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python tensorflow/models/research/object_detection/model_main_tf2.py --model_dir=tensorflow/workspace/models/my_ssd_mobnet --pipeline_config_path=tensorflow/workspace/models/my_ssd_mobnet/pipeline.config --num_train_steps=1000\n"
          ]
        }
      ],
      "source": [
        "print(command)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "config = ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = InteractiveSession(config=config)"
      ],
      "metadata": {
        "id": "w6g5t8YG0x32"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "lWn2mkJldG7R",
        "outputId": "e692084e-0d6a-4d57-c3e9-0322348b1c7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-16 19:10:15.971559: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/usr/local/lib/python3.10/dist-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
            "caused by: ['/usr/local/lib/python3.10/dist-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6Status12empty_stringB5cxx11Ev']\n",
            "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/usr/local/lib/python3.10/dist-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
            "caused by: ['/usr/local/lib/python3.10/dist-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZNK10tensorflow4data11DatasetBase8FinalizeEPNS_15OpKernelContextESt8functionIFN3tsl8StatusOrISt10unique_ptrIS1_NS5_4core15RefCountDeleterEEEEvEE']\n",
            "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
            "I1016 19:10:21.674939 132775515115520 mirrored_strategy.py:419] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
            "INFO:tensorflow:Maybe overwriting train_steps: 1000\n",
            "I1016 19:10:22.170851 132775515115520 config_util.py:552] Maybe overwriting train_steps: 1000\n",
            "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
            "I1016 19:10:22.171272 132775515115520 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "rename to distribute_datasets_from_function\n",
            "W1016 19:10:22.253053 132775515115520 deprecation.py:364] From /usr/local/lib/python3.10/dist-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "rename to distribute_datasets_from_function\n",
            "INFO:tensorflow:Reading unweighted datasets: ['tensorflow/workspace/annotations/train.record']\n",
            "I1016 19:10:22.285818 132775515115520 dataset_builder.py:162] Reading unweighted datasets: ['tensorflow/workspace/annotations/train.record']\n",
            "INFO:tensorflow:Reading record datasets for input file: ['tensorflow/workspace/annotations/train.record']\n",
            "I1016 19:10:22.286391 132775515115520 dataset_builder.py:79] Reading record datasets for input file: ['tensorflow/workspace/annotations/train.record']\n",
            "INFO:tensorflow:Number of filenames to read: 1\n",
            "I1016 19:10:22.286582 132775515115520 dataset_builder.py:80] Number of filenames to read: 1\n",
            "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
            "W1016 19:10:22.286688 132775515115520 dataset_builder.py:86] num_readers has been reduced to 1 to match input file shards.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
            "W1016 19:10:22.328345 132775515115520 deprecation.py:364] From /usr/local/lib/python3.10/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W1016 19:10:22.389404 132775515115520 deprecation.py:364] From /usr/local/lib/python3.10/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "W1016 19:10:33.393048 132775515115520 deprecation.py:364] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "W1016 19:10:37.306880 132775515115520 deprecation.py:364] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W1016 19:10:40.711740 132775515115520 deprecation.py:364] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "2023-10-16 19:10:47.311378: W tensorflow/core/framework/dataset.cc:956] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend.py:452: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn(\n",
            "I1016 19:11:19.566536 132772739937856 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
            "I1016 19:11:34.358412 132772739937856 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
            "2023-10-16 19:11:43.608758: W tensorflow/core/framework/dataset.cc:956] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/deprecation.py:648: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n",
            "W1016 19:11:44.945732 132773246400064 deprecation.py:569] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/deprecation.py:648: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n",
            "I1016 19:11:47.325989 132773246400064 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
            "I1016 19:11:58.001643 132773246400064 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
            "I1016 19:12:04.927038 132773246400064 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
            "I1016 19:12:14.733898 132773246400064 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!{command}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "DwYx262fdG7R",
        "outputId": "c2873fe8-de8d-47a2-8b58-44058bcc571d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gin-config==0.1.1\n",
            "  Downloading gin-config-0.1.1.tar.gz (40 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from gin-config==0.1.1) (1.16.0)\n",
            "Building wheels for collected packages: gin-config\n",
            "  Building wheel for gin-config (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gin-config: filename=gin_config-0.1.1-py3-none-any.whl size=38308 sha256=a33d1ac637ecde19605b590271c2b196017e5436403b911c773f0cb9773aee4f\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/ad/a7/b085ddc60427ebf0d52e1643eb4906911866690c004adbcc9d\n",
            "Successfully built gin-config\n",
            "Installing collected packages: gin-config\n",
            "  Attempting uninstall: gin-config\n",
            "    Found existing installation: gin-config 0.5.0\n",
            "    Uninstalling gin-config-0.5.0:\n",
            "      Successfully uninstalled gin-config-0.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.0.6 requires gin-config>=0.3.0, but you have gin-config 0.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gin-config-0.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gin-config==0.1.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlmUFvOzdG7R"
      },
      "source": [
        "\n",
        "# 4. **Detecting Objects**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7X9RCdPdG7R"
      },
      "source": [
        "\n",
        "# 5. **Freezing and Conversion**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaP3T41PdG7S"
      },
      "source": [
        "\n",
        "# 6. **Performance Tuning**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOyaXaw7dG7S"
      },
      "source": [
        "\n",
        "# 7. **Packaging**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFKO8Z50dG7S"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8umfmSCdG7S"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}